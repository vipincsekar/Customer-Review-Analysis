# -*- coding: utf-8 -*-
"""Customer Review Vipin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AP_gyJT0YdNgJ0-VQZIqwjPTMHD9q1qt
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt

data = pd.read_csv("/content/dataset.csv", encoding='ISO-8859-1')

data.head(2)

data.shape

data["Location"].value_counts()

corr_matrix = data.corr()
corr_matrix["Rating"].sort_values(ascending=False)

data["date_time"] =pd.to_datetime(data["date_time"])

def add_datepart1(df, fldname, drop=True, time=False, errors="raise"):	

    fld = df[fldname]
    fld_dtype = fld.dtype
    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):
        fld_dtype = np.datetime64

    if not np.issubdtype(fld_dtype, np.datetime64):
        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)
    targ_pre = re.sub('[Dd]ate$', '', fldname)
    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',
            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']
    if time: attr = attr + ['Hour', 'Minute', 'Second']
    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())
    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9
    if drop: df.drop(fldname, axis=1, inplace=True)

def is_date(x): return np.issubdtype(x.dtype, np.datetime64)

add_datepart1(data,"date_time")
data.head()

data.head(2)

from pandas.tools.plotting import scatter_matrix
attributes =["Rating","Views","Total_reviews_by_customer","Customer_follower"]
scatter_matrix(data[attributes], figsize=(12,8))

import seaborn as sns
sns.heatmap(data.corr(),cmap='YlGnBu')

"""**Sentiment Analysis on the attribute "Subject"**"""

data["class"]=np.zeros([data.shape[0],1])
data["class"].loc[data.Rating>=3]=1
data["class"].head(2)

X = data["subject"]
y = data["class"]

"""**Training and testing split is performed as 80-20%**"""

#Train test split
from sklearn.model_selection import train_test_split
Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size=0.2)
Xtrain.shape, ytrain.shape, Xval.shape, yval.shape

data["class"].hist()

from sklearn.feature_extraction.text import CountVectorizer

"""**The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.**

**Stopword-a word that is automatically omitted from a computer-generated concordance or index.**
**Example : is,the **
**These stopwords are removed from the analysis**
"""

veczr = CountVectorizer(ngram_range=(1,3 ), lowercase = True, stop_words='english')

veczr.fit(Xtrain)
trn_term = veczr.transform(Xtrain)
val_term = veczr.transform(Xval)

"""**Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables**"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(C=0.1, dual=True)
model.fit(trn_term, ytrain)

prediction = model.predict(trn_term)
prediction_val = model.predict(val_term)
np.sum(prediction==ytrain)/len(ytrain), np.sum(prediction_val==yval)/len(yval)

from sklearn.metrics import confusion_matrix 
confusion_matrix( ytrain, prediction)

"""**Inference : 84 values are predicted correctly and 20 or not.**"""

from sklearn.ensemble import RandomForestClassifier
model_RF = RandomForestClassifier(n_estimators=10)

Xt = trn_term.sign()
Xv = val_term.sign()

model_RF.fit(Xt, ytrain)

prediction = model_RF.predict(Xt)
prediction_val = model_RF.predict(Xv)

np.sum(prediction==ytrain)/len(ytrain), np.sum(prediction_val==yval)/len(yval)

from sklearn.model_selection import GridSearchCV
param_grid = [
    {'n_estimators': [3,10,30], 'max_features': [2,4,6,8]},
    {'bootstrap' : [False],'n_estimators': [3,10], 'max_features': [2, 3, 4]}
]

grid_search = GridSearchCV(model_RF, param_grid, cv=5, scoring='accuracy')
grid_search.fit(trn_term, ytrain)

grid_search.best_params_

grid_search.cv_results_

prediction = grid_search.predict(Xt)
prediction_val = grid_search.predict(Xv)
np.sum(prediction==ytrain)/len(ytrain), np.sum(prediction_val==yval)/len(yval)

